2025-10-15 18:50:50,616 - nanochat.checkpoint_manager - [32m[1mINFO[0m - No model tag provided, guessing model tag: d20
2025-10-15 18:50:50,617 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Loading model from /root/.cache/nanochat/mid_checkpoints/d20 with step 765
2025-10-15 18:50:51,731 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}
Target examples per step: 32
Device batch size: 4
Examples per step is device_batch_size * ddp_world_size: 32
=> Setting grad accum steps: 1
Scaling the LR for the AdamW parameters ‚àù1/‚àö(1280/768) = 0.774597
Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32
Step 00000 | Validation loss: 1.079310
Step 00000/00651 | Training loss: 0.956052| lrm: 1.000000| num_tokens: 15,432
Step 00001/00651 | Training loss: 0.944759| lrm: 0.998464| num_tokens: 15,909
Step 00002/00651 | Training loss: 0.721290| lrm: 0.996928| num_tokens: 11,357
Step 00003/00651 | Training loss: 1.329755| lrm: 0.995392| num_tokens: 16,114
Step 00004/00651 | Training loss: 1.263507| lrm: 0.993856| num_tokens: 14,531
Step 00005/00651 | Training loss: 1.111042| lrm: 0.992320| num_tokens: 10,297
Step 00006/00651 | Training loss: 1.009765| lrm: 0.990783| num_tokens: 10,573
Step 00007/00651 | Training loss: 1.414289| lrm: 0.989247| num_tokens: 15,292
Step 00008/00651 | Training loss: 1.161484| lrm: 0.987711| num_tokens: 9,404
Step 00009/00651 | Training loss: 0.827724| lrm: 0.986175| num_tokens: 5,531
Step 00010/00651 | Training loss: 0.577117| lrm: 0.984639| num_tokens: 11,459
Step 00011/00651 | Training loss: 0.978753| lrm: 0.983103| num_tokens: 11,072
Step 00012/00651 | Training loss: 1.485756| lrm: 0.981567| num_tokens: 6,950
Step 00013/00651 | Training loss: 1.004325| lrm: 0.980031| num_tokens: 11,350
Step 00014/00651 | Training loss: 1.132172| lrm: 0.978495| num_tokens: 13,595
Step 00015/00651 | Training loss: 0.779517| lrm: 0.976959| num_tokens: 7,334
Step 00016/00651 | Training loss: 1.074140| lrm: 0.975422| num_tokens: 11,172
Step 00017/00651 | Training loss: 0.791637| lrm: 0.973886| num_tokens: 8,160
Step 00018/00651 | Training loss: 1.093641| lrm: 0.972350| num_tokens: 13,808
Step 00019/00651 | Training loss: 0.378295| lrm: 0.970814| num_tokens: 9,144
Step 00020/00651 | Training loss: 0.424352| lrm: 0.969278| num_tokens: 13,832
Step 00021/00651 | Training loss: 0.912989| lrm: 0.967742| num_tokens: 8,456
Step 00022/00651 | Training loss: 0.960661| lrm: 0.966206| num_tokens: 7,237
Step 00023/00651 | Training loss: 0.516154| lrm: 0.964670| num_tokens: 8,932
Step 00024/00651 | Training loss: 0.855904| lrm: 0.963134| num_tokens: 11,920
Step 00025/00651 | Training loss: 0.638800| lrm: 0.961598| num_tokens: 11,861
Step 00026/00651 | Training loss: 1.142938| lrm: 0.960061| num_tokens: 12,663
Step 00027/00651 | Training loss: 1.118324| lrm: 0.958525| num_tokens: 9,776
Step 00028/00651 | Training loss: 0.939675| lrm: 0.956989| num_tokens: 10,586
Step 00029/00651 | Training loss: 0.561548| lrm: 0.955453| num_tokens: 10,399
Step 00030/00651 | Training loss: 0.416892| lrm: 0.953917| num_tokens: 11,440
Step 00031/00651 | Training loss: 0.906675| lrm: 0.952381| num_tokens: 7,149
Step 00032/00651 | Training loss: 1.058299| lrm: 0.950845| num_tokens: 10,146
Step 00033/00651 | Training loss: 0.802644| lrm: 0.949309| num_tokens: 13,299
Step 00034/00651 | Training loss: 0.904548| lrm: 0.947773| num_tokens: 16,126
Step 00035/00651 | Training loss: 0.897334| lrm: 0.946237| num_tokens: 10,816
Step 00036/00651 | Training loss: 0.801559| lrm: 0.944700| num_tokens: 11,641
Step 00037/00651 | Training loss: 1.357676| lrm: 0.943164| num_tokens: 10,465
Step 00038/00651 | Training loss: 1.199187| lrm: 0.941628| num_tokens: 11,988
Step 00039/00651 | Training loss: 1.045616| lrm: 0.940092| num_tokens: 12,624
Step 00040/00651 | Training loss: 0.973095| lrm: 0.938556| num_tokens: 10,752
Step 00041/00651 | Training loss: 0.626063| lrm: 0.937020| num_tokens: 7,080
Step 00042/00651 | Training loss: 1.162852| lrm: 0.935484| num_tokens: 11,835
Step 00043/00651 | Training loss: 1.279008| lrm: 0.933948| num_tokens: 13,249
Step 00044/00651 | Training loss: 0.515278| lrm: 0.932412| num_tokens: 8,708
Step 00045/00651 | Training loss: 1.426125| lrm: 0.930876| num_tokens: 11,727
Step 00046/00651 | Training loss: 1.130333| lrm: 0.929339| num_tokens: 15,331
Step 00047/00651 | Training loss: 1.136087| lrm: 0.927803| num_tokens: 11,482
Step 00048/00651 | Training loss: 0.971476| lrm: 0.926267| num_tokens: 13,118
Step 00049/00651 | Training loss: 1.030427| lrm: 0.924731| num_tokens: 6,116
Step 00050/00651 | Training loss: 1.509271| lrm: 0.923195| num_tokens: 15,859
Step 00051/00651 | Training loss: 0.792049| lrm: 0.921659| num_tokens: 8,193
Step 00052/00651 | Training loss: 0.901633| lrm: 0.920123| num_tokens: 12,343
Step 00053/00651 | Training loss: 0.951668| lrm: 0.918587| num_tokens: 11,690
Step 00054/00651 | Training loss: 0.836545| lrm: 0.917051| num_tokens: 10,379
Step 00055/00651 | Training loss: 0.486240| lrm: 0.915515| num_tokens: 12,278
Step 00056/00651 | Training loss: 0.922914| lrm: 0.913978| num_tokens: 12,330
Step 00057/00651 | Training loss: 0.818013| lrm: 0.912442| num_tokens: 12,984
Step 00058/00651 | Training loss: 0.808999| lrm: 0.910906| num_tokens: 10,527
Step 00059/00651 | Training loss: 1.005287| lrm: 0.909370| num_tokens: 10,475
Step 00060/00651 | Training loss: 0.416205| lrm: 0.907834| num_tokens: 12,859
Step 00061/00651 | Training loss: 0.849342| lrm: 0.906298| num_tokens: 9,833
Step 00062/00651 | Training loss: 1.124070| lrm: 0.904762| num_tokens: 14,447
Step 00063/00651 | Training loss: 1.053927| lrm: 0.903226| num_tokens: 12,273
Step 00064/00651 | Training loss: 1.089871| lrm: 0.901690| num_tokens: 9,206
Step 00065/00651 | Training loss: 0.351285| lrm: 0.900154| num_tokens: 8,682
Step 00066/00651 | Training loss: 0.597859| lrm: 0.898618| num_tokens: 11,241
Step 00067/00651 | Training loss: 0.519913| lrm: 0.897081| num_tokens: 11,320
Step 00068/00651 | Training loss: 0.552297| lrm: 0.895545| num_tokens: 15,531
Step 00069/00651 | Training loss: 1.176723| lrm: 0.894009| num_tokens: 10,749
Step 00070/00651 | Training loss: 1.180624| lrm: 0.892473| num_tokens: 8,719
Step 00071/00651 | Training loss: 0.789052| lrm: 0.890937| num_tokens: 11,843
Step 00072/00651 | Training loss: 0.564170| lrm: 0.889401| num_tokens: 11,034
Step 00073/00651 | Training loss: 0.616022| lrm: 0.887865| num_tokens: 11,168
Step 00074/00651 | Training loss: 0.472454| lrm: 0.886329| num_tokens: 9,327
Step 00075/00651 | Training loss: 1.025982| lrm: 0.884793| num_tokens: 11,488
Step 00076/00651 | Training loss: 0.506501| lrm: 0.883257| num_tokens: 11,683
Step 00077/00651 | Training loss: 0.482254| lrm: 0.881720| num_tokens: 7,865
Step 00078/00651 | Training loss: 1.211899| lrm: 0.880184| num_tokens: 11,109
Step 00079/00651 | Training loss: 0.898574| lrm: 0.878648| num_tokens: 12,100
Step 00080/00651 | Training loss: 0.752106| lrm: 0.877112| num_tokens: 13,191
Step 00081/00651 | Training loss: 0.718020| lrm: 0.875576| num_tokens: 5,431
Step 00082/00651 | Training loss: 0.472903| lrm: 0.874040| num_tokens: 11,225
Step 00083/00651 | Training loss: 0.679649| lrm: 0.872504| num_tokens: 11,762
Step 00084/00651 | Training loss: 0.845504| lrm: 0.870968| num_tokens: 10,098
Step 00085/00651 | Training loss: 0.547876| lrm: 0.869432| num_tokens: 8,880
Step 00086/00651 | Training loss: 0.764760| lrm: 0.867896| num_tokens: 8,880
Step 00087/00651 | Training loss: 1.152976| lrm: 0.866359| num_tokens: 15,919
Step 00088/00651 | Training loss: 0.861421| lrm: 0.864823| num_tokens: 10,951
Step 00089/00651 | Training loss: 1.005286| lrm: 0.863287| num_tokens: 12,086
Step 00090/00651 | Training loss: 1.129158| lrm: 0.861751| num_tokens: 8,962
Step 00091/00651 | Training loss: 1.097747| lrm: 0.860215| num_tokens: 14,056
Step 00092/00651 | Training loss: 0.924260| lrm: 0.858679| num_tokens: 10,541
Step 00093/00651 | Training loss: 1.189611| lrm: 0.857143| num_tokens: 9,797
Step 00094/00651 | Training loss: 1.120441| lrm: 0.855607| num_tokens: 9,610
Step 00095/00651 | Training loss: 0.429911| lrm: 0.854071| num_tokens: 9,648
Step 00096/00651 | Training loss: 0.853546| lrm: 0.852535| num_tokens: 10,512
Step 00097/00651 | Training loss: 0.535852| lrm: 0.850998| num_tokens: 8,601
Step 00098/00651 | Training loss: 0.838281| lrm: 0.849462| num_tokens: 11,927
Step 00099/00651 | Training loss: 1.393420| lrm: 0.847926| num_tokens: 12,097
Step 00100 | Validation loss: 1.073468
Step 00100/00651 | Training loss: 1.264318| lrm: 0.846390| num_tokens: 11,126
Step 00101/00651 | Training loss: 0.761893| lrm: 0.844854| num_tokens: 10,333
Step 00102/00651 | Training loss: 0.522234| lrm: 0.843318| num_tokens: 9,697
Step 00103/00651 | Training loss: 0.825811| lrm: 0.841782| num_tokens: 14,321
Step 00104/00651 | Training loss: 0.853640| lrm: 0.840246| num_tokens: 10,257
Step 00105/00651 | Training loss: 0.882799| lrm: 0.838710| num_tokens: 9,327
Step 00106/00651 | Training loss: 0.772371| lrm: 0.837174| num_tokens: 9,114
Step 00107/00651 | Training loss: 0.845624| lrm: 0.835637| num_tokens: 11,792
Step 00108/00651 | Training loss: 0.985303| lrm: 0.834101| num_tokens: 12,747
Step 00109/00651 | Training loss: 1.006019| lrm: 0.832565| num_tokens: 10,286
Step 00110/00651 | Training loss: 1.025630| lrm: 0.831029| num_tokens: 8,810
Step 00111/00651 | Training loss: 0.978943| lrm: 0.829493| num_tokens: 12,371
Step 00112/00651 | Training loss: 0.689002| lrm: 0.827957| num_tokens: 10,294
Step 00113/00651 | Training loss: 0.920533| lrm: 0.826421| num_tokens: 10,437
Step 00114/00651 | Training loss: 0.749597| lrm: 0.824885| num_tokens: 11,436
Step 00115/00651 | Training loss: 0.786855| lrm: 0.823349| num_tokens: 11,040
Step 00116/00651 | Training loss: 0.630912| lrm: 0.821813| num_tokens: 6,876
Step 00117/00651 | Training loss: 1.244051| lrm: 0.820276| num_tokens: 15,373
Step 00118/00651 | Training loss: 0.792176| lrm: 0.818740| num_tokens: 8,086
Step 00119/00651 | Training loss: 1.181469| lrm: 0.817204| num_tokens: 9,283
Step 00120/00651 | Training loss: 1.073173| lrm: 0.815668| num_tokens: 10,481
Step 00121/00651 | Training loss: 0.937216| lrm: 0.814132| num_tokens: 12,038
Step 00122/00651 | Training loss: 1.004483| lrm: 0.812596| num_tokens: 10,663
Step 00123/00651 | Training loss: 0.945848| lrm: 0.811060| num_tokens: 13,228
Step 00124/00651 | Training loss: 0.474031| lrm: 0.809524| num_tokens: 10,014
Step 00125/00651 | Training loss: 1.101261| lrm: 0.807988| num_tokens: 5,172
Step 00126/00651 | Training loss: 0.943711| lrm: 0.806452| num_tokens: 14,668
Step 00127/00651 | Training loss: 1.036700| lrm: 0.804916| num_tokens: 7,599
Step 00128/00651 | Training loss: 0.847434| lrm: 0.803379| num_tokens: 12,108
Step 00129/00651 | Training loss: 0.913405| lrm: 0.801843| num_tokens: 10,599
Step 00130/00651 | Training loss: 0.917888| lrm: 0.800307| num_tokens: 12,135
Step 00131/00651 | Training loss: 1.017648| lrm: 0.798771| num_tokens: 10,156
Step 00132/00651 | Training loss: 0.767135| lrm: 0.797235| num_tokens: 9,018
Step 00133/00651 | Training loss: 0.795489| lrm: 0.795699| num_tokens: 12,475
Step 00134/00651 | Training loss: 1.071490| lrm: 0.794163| num_tokens: 9,008
Step 00135/00651 | Training loss: 1.215436| lrm: 0.792627| num_tokens: 9,279
Step 00136/00651 | Training loss: 1.507295| lrm: 0.791091| num_tokens: 8,933
Step 00137/00651 | Training loss: 1.434823| lrm: 0.789555| num_tokens: 6,565
Step 00138/00651 | Training loss: 0.629981| lrm: 0.788018| num_tokens: 7,465
Step 00139/00651 | Training loss: 1.139410| lrm: 0.786482| num_tokens: 5,889
Step 00140/00651 | Training loss: 0.708253| lrm: 0.784946| num_tokens: 9,387
Step 00141/00651 | Training loss: 1.349535| lrm: 0.783410| num_tokens: 9,859
Step 00142/00651 | Training loss: 0.597378| lrm: 0.781874| num_tokens: 8,651
Step 00143/00651 | Training loss: 1.075104| lrm: 0.780338| num_tokens: 8,860
Step 00144/00651 | Training loss: 0.916076| lrm: 0.778802| num_tokens: 12,254
Step 00145/00651 | Training loss: 1.146053| lrm: 0.777266| num_tokens: 11,979
Step 00146/00651 | Training loss: 0.857795| lrm: 0.775730| num_tokens: 9,719
Step 00147/00651 | Training loss: 0.771577| lrm: 0.774194| num_tokens: 12,817
Step 00148/00651 | Training loss: 1.249620| lrm: 0.772657| num_tokens: 11,309
Step 00149/00651 | Training loss: 1.398116| lrm: 0.771121| num_tokens: 9,483
Step 00150/00651 | Training loss: 1.364335| lrm: 0.769585| num_tokens: 11,821
Step 00151/00651 | Training loss: 0.901764| lrm: 0.768049| num_tokens: 10,072
Step 00152/00651 | Training loss: 0.917661| lrm: 0.766513| num_tokens: 8,005
Step 00153/00651 | Training loss: 1.113017| lrm: 0.764977| num_tokens: 8,914
Step 00154/00651 | Training loss: 1.051183| lrm: 0.763441| num_tokens: 12,238
Step 00155/00651 | Training loss: 0.796066| lrm: 0.761905| num_tokens: 8,165
Step 00156/00651 | Training loss: 0.715467| lrm: 0.760369| num_tokens: 7,980
Step 00157/00651 | Training loss: 1.187124| lrm: 0.758833| num_tokens: 8,973
Step 00158/00651 | Training loss: 0.800402| lrm: 0.757296| num_tokens: 9,234
Step 00159/00651 | Training loss: 0.643767| lrm: 0.755760| num_tokens: 8,496
Step 00160/00651 | Training loss: 0.898299| lrm: 0.754224| num_tokens: 5,978
Step 00161/00651 | Training loss: 0.790737| lrm: 0.752688| num_tokens: 8,493
Step 00162/00651 | Training loss: 0.641810| lrm: 0.751152| num_tokens: 10,330
Step 00163/00651 | Training loss: 0.876092| lrm: 0.749616| num_tokens: 10,131
Step 00164/00651 | Training loss: 0.999123| lrm: 0.748080| num_tokens: 12,624
Step 00165/00651 | Training loss: 0.590900| lrm: 0.746544| num_tokens: 10,185
Step 00166/00651 | Training loss: 0.976391| lrm: 0.745008| num_tokens: 10,167
Step 00167/00651 | Training loss: 1.048809| lrm: 0.743472| num_tokens: 8,453
Step 00168/00651 | Training loss: 0.946269| lrm: 0.741935| num_tokens: 8,979
Step 00169/00651 | Training loss: 1.126392| lrm: 0.740399| num_tokens: 11,339
Step 00170/00651 | Training loss: 0.875010| lrm: 0.738863| num_tokens: 11,715
Step 00171/00651 | Training loss: 0.553788| lrm: 0.737327| num_tokens: 9,023
Step 00172/00651 | Training loss: 0.750212| lrm: 0.735791| num_tokens: 9,598
Step 00173/00651 | Training loss: 0.587263| lrm: 0.734255| num_tokens: 11,903
Step 00174/00651 | Training loss: 0.809847| lrm: 0.732719| num_tokens: 9,932
Step 00175/00651 | Training loss: 0.686012| lrm: 0.731183| num_tokens: 9,019
Step 00176/00651 | Training loss: 0.880143| lrm: 0.729647| num_tokens: 11,524
Step 00177/00651 | Training loss: 1.102800| lrm: 0.728111| num_tokens: 14,665
Step 00178/00651 | Training loss: 1.081057| lrm: 0.726575| num_tokens: 10,083
Step 00179/00651 | Training loss: 1.388417| lrm: 0.725038| num_tokens: 7,040
Step 00180/00651 | Training loss: 1.386403| lrm: 0.723502| num_tokens: 11,185
Step 00181/00651 | Training loss: 1.242911| lrm: 0.721966| num_tokens: 14,478
Step 00182/00651 | Training loss: 0.784363| lrm: 0.720430| num_tokens: 14,492
Step 00183/00651 | Training loss: 0.979394| lrm: 0.718894| num_tokens: 15,861
Step 00184/00651 | Training loss: 0.888834| lrm: 0.717358| num_tokens: 11,135
Step 00185/00651 | Training loss: 1.207065| lrm: 0.715822| num_tokens: 10,823
Step 00186/00651 | Training loss: 0.772959| lrm: 0.714286| num_tokens: 12,346
Step 00187/00651 | Training loss: 0.995173| lrm: 0.712750| num_tokens: 10,587
Step 00188/00651 | Training loss: 0.876934| lrm: 0.711214| num_tokens: 10,827
Step 00189/00651 | Training loss: 1.294811| lrm: 0.709677| num_tokens: 9,051
Step 00190/00651 | Training loss: 0.746792| lrm: 0.708141| num_tokens: 12,362
Step 00191/00651 | Training loss: 0.423693| lrm: 0.706605| num_tokens: 13,422
Step 00192/00651 | Training loss: 0.764874| lrm: 0.705069| num_tokens: 6,248
Step 00193/00651 | Training loss: 0.643578| lrm: 0.703533| num_tokens: 10,422
Step 00194/00651 | Training loss: 0.926971| lrm: 0.701997| num_tokens: 9,799
Step 00195/00651 | Training loss: 0.839598| lrm: 0.700461| num_tokens: 10,977
Step 00196/00651 | Training loss: 0.659859| lrm: 0.698925| num_tokens: 11,765
Step 00197/00651 | Training loss: 1.077299| lrm: 0.697389| num_tokens: 8,940
Step 00198/00651 | Training loss: 0.809169| lrm: 0.695853| num_tokens: 11,173
Step 00199/00651 | Training loss: 0.627241| lrm: 0.694316| num_tokens: 7,835
Step 00200 | Validation loss: 1.071190
Final: 261/1024 (25.49%)
Final: 269/1024 (26.27%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 5/64 (7.81%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 2/64 (3.12%)
Step 00200 | mmlu_acc: 0.254883, arc_easy_acc: 0.262695, gsm8k_acc: 0.078125, humaneval_acc: 0.031250
Step 00200/00651 | Training loss: 1.174435| lrm: 0.692780| num_tokens: 15,427
Step 00201/00651 | Training loss: 1.019555| lrm: 0.691244| num_tokens: 13,861
Step 00202/00651 | Training loss: 1.190963| lrm: 0.689708| num_tokens: 7,064
Step 00203/00651 | Training loss: 1.141695| lrm: 0.688172| num_tokens: 9,201
Step 00204/00651 | Training loss: 0.728438| lrm: 0.686636| num_tokens: 10,893
Step 00205/00651 | Training loss: 1.196743| lrm: 0.685100| num_tokens: 12,916
Step 00206/00651 | Training loss: 1.023886| lrm: 0.683564| num_tokens: 13,471
Step 00207/00651 | Training loss: 1.098711| lrm: 0.682028| num_tokens: 11,746
Step 00208/00651 | Training loss: 0.573796| lrm: 0.680492| num_tokens: 9,903
Step 00209/00651 | Training loss: 0.931586| lrm: 0.678955| num_tokens: 10,800
Step 00210/00651 | Training loss: 1.033769| lrm: 0.677419| num_tokens: 8,371
Step 00211/00651 | Training loss: 0.754897| lrm: 0.675883| num_tokens: 10,278
Step 00212/00651 | Training loss: 1.078963| lrm: 0.674347| num_tokens: 15,688
Step 00213/00651 | Training loss: 1.023518| lrm: 0.672811| num_tokens: 13,662
Step 00214/00651 | Training loss: 0.822645| lrm: 0.671275| num_tokens: 10,501
Step 00215/00651 | Training loss: 1.081066| lrm: 0.669739| num_tokens: 13,927
Step 00216/00651 | Training loss: 0.874277| lrm: 0.668203| num_tokens: 12,842
Step 00217/00651 | Training loss: 0.694166| lrm: 0.666667| num_tokens: 13,601
Step 00218/00651 | Training loss: 0.569389| lrm: 0.665131| num_tokens: 14,640
Step 00219/00651 | Training loss: 1.058063| lrm: 0.663594| num_tokens: 10,602
Step 00220/00651 | Training loss: 0.988357| lrm: 0.662058| num_tokens: 10,435
Step 00221/00651 | Training loss: 0.585530| lrm: 0.660522| num_tokens: 9,645
Step 00222/00651 | Training loss: 1.045506| lrm: 0.658986| num_tokens: 14,785
Step 00223/00651 | Training loss: 0.916679| lrm: 0.657450| num_tokens: 7,589
Step 00224/00651 | Training loss: 1.221991| lrm: 0.655914| num_tokens: 13,605
Step 00225/00651 | Training loss: 0.671624| lrm: 0.654378| num_tokens: 10,401
Step 00226/00651 | Training loss: 1.365010| lrm: 0.652842| num_tokens: 8,125
Step 00227/00651 | Training loss: 1.663547| lrm: 0.651306| num_tokens: 6,355
Step 00228/00651 | Training loss: 0.896643| lrm: 0.649770| num_tokens: 13,460
Step 00229/00651 | Training loss: 0.754293| lrm: 0.648233| num_tokens: 10,081
Step 00230/00651 | Training loss: 1.097548| lrm: 0.646697| num_tokens: 10,510
Step 00231/00651 | Training loss: 0.796155| lrm: 0.645161| num_tokens: 10,362
Step 00232/00651 | Training loss: 1.073660| lrm: 0.643625| num_tokens: 12,835
Step 00233/00651 | Training loss: 0.716219| lrm: 0.642089| num_tokens: 12,461
Step 00234/00651 | Training loss: 0.953862| lrm: 0.640553| num_tokens: 12,739
Step 00235/00651 | Training loss: 0.935484| lrm: 0.639017| num_tokens: 11,577
Step 00236/00651 | Training loss: 0.849718| lrm: 0.637481| num_tokens: 8,758
Step 00237/00651 | Training loss: 1.155887| lrm: 0.635945| num_tokens: 11,138
Step 00238/00651 | Training loss: 1.057164| lrm: 0.634409| num_tokens: 13,313
Step 00239/00651 | Training loss: 1.003471| lrm: 0.632873| num_tokens: 14,561
Step 00240/00651 | Training loss: 0.988659| lrm: 0.631336| num_tokens: 5,044
Step 00241/00651 | Training loss: 0.997582| lrm: 0.629800| num_tokens: 10,634
Step 00242/00651 | Training loss: 0.936794| lrm: 0.628264| num_tokens: 9,593
Step 00243/00651 | Training loss: 0.992962| lrm: 0.626728| num_tokens: 15,037
Step 00244/00651 | Training loss: 0.513734| lrm: 0.625192| num_tokens: 10,743
Step 00245/00651 | Training loss: 0.599346| lrm: 0.623656| num_tokens: 7,519
Step 00246/00651 | Training loss: 1.510608| lrm: 0.622120| num_tokens: 11,970
Step 00247/00651 | Training loss: 1.073878| lrm: 0.620584| num_tokens: 7,465
Step 00248/00651 | Training loss: 1.191706| lrm: 0.619048| num_tokens: 16,307
Step 00249/00651 | Training loss: 0.705607| lrm: 0.617512| num_tokens: 12,285
Step 00250/00651 | Training loss: 1.195968| lrm: 0.615975| num_tokens: 10,597
Step 00251/00651 | Training loss: 1.442510| lrm: 0.614439| num_tokens: 14,200
Step 00252/00651 | Training loss: 0.995461| lrm: 0.612903| num_tokens: 10,098
Step 00253/00651 | Training loss: 1.200255| lrm: 0.611367| num_tokens: 13,074
Step 00254/00651 | Training loss: 0.872336| lrm: 0.609831| num_tokens: 11,801
Step 00255/00651 | Training loss: 0.928042| lrm: 0.608295| num_tokens: 11,264
Step 00256/00651 | Training loss: 1.161792| lrm: 0.606759| num_tokens: 14,341
Step 00257/00651 | Training loss: 0.719669| lrm: 0.605223| num_tokens: 10,064
Step 00258/00651 | Training loss: 0.839782| lrm: 0.603687| num_tokens: 8,193
Step 00259/00651 | Training loss: 0.676392| lrm: 0.602151| num_tokens: 7,392
Step 00260/00651 | Training loss: 0.740502| lrm: 0.600614| num_tokens: 15,345
Step 00261/00651 | Training loss: 0.581154| lrm: 0.599078| num_tokens: 8,772
Step 00262/00651 | Training loss: 0.915389| lrm: 0.597542| num_tokens: 9,810
Step 00263/00651 | Training loss: 0.696575| lrm: 0.596006| num_tokens: 13,391
Step 00264/00651 | Training loss: 1.310445| lrm: 0.594470| num_tokens: 14,255
Step 00265/00651 | Training loss: 0.910332| lrm: 0.592934| num_tokens: 13,350
Step 00266/00651 | Training loss: 1.089526| lrm: 0.591398| num_tokens: 8,463
Step 00267/00651 | Training loss: 1.085344| lrm: 0.589862| num_tokens: 15,478
Step 00268/00651 | Training loss: 0.703038| lrm: 0.588326| num_tokens: 12,615
Step 00269/00651 | Training loss: 0.919202| lrm: 0.586790| num_tokens: 12,023
Step 00270/00651 | Training loss: 1.311776| lrm: 0.585253| num_tokens: 11,693
Step 00271/00651 | Training loss: 0.900292| lrm: 0.583717| num_tokens: 13,298
Step 00272/00651 | Training loss: 1.030694| lrm: 0.582181| num_tokens: 10,061
Step 00273/00651 | Training loss: 0.930728| lrm: 0.580645| num_tokens: 15,225
Step 00274/00651 | Training loss: 1.161924| lrm: 0.579109| num_tokens: 10,683
Step 00275/00651 | Training loss: 0.740988| lrm: 0.577573| num_tokens: 5,649
Step 00276/00651 | Training loss: 0.867037| lrm: 0.576037| num_tokens: 8,625
Step 00277/00651 | Training loss: 1.071430| lrm: 0.574501| num_tokens: 10,297
Step 00278/00651 | Training loss: 1.024062| lrm: 0.572965| num_tokens: 14,391
Step 00279/00651 | Training loss: 1.291551| lrm: 0.571429| num_tokens: 15,615
Step 00280/00651 | Training loss: 1.090751| lrm: 0.569892| num_tokens: 14,387
Step 00281/00651 | Training loss: 0.919327| lrm: 0.568356| num_tokens: 7,488
Step 00282/00651 | Training loss: 1.059373| lrm: 0.566820| num_tokens: 17,169
Step 00283/00651 | Training loss: 0.904150| lrm: 0.565284| num_tokens: 11,091
Step 00284/00651 | Training loss: 0.897166| lrm: 0.563748| num_tokens: 8,554
Step 00285/00651 | Training loss: 0.783884| lrm: 0.562212| num_tokens: 9,086
Step 00286/00651 | Training loss: 0.763817| lrm: 0.560676| num_tokens: 13,440
Step 00287/00651 | Training loss: 0.764977| lrm: 0.559140| num_tokens: 9,593
Step 00288/00651 | Training loss: 0.811366| lrm: 0.557604| num_tokens: 11,294
Step 00289/00651 | Training loss: 0.823049| lrm: 0.556068| num_tokens: 7,891
Step 00290/00651 | Training loss: 0.978418| lrm: 0.554531| num_tokens: 10,713
Step 00291/00651 | Training loss: 0.757853| lrm: 0.552995| num_tokens: 9,717
Step 00292/00651 | Training loss: 0.586458| lrm: 0.551459| num_tokens: 7,198
Step 00293/00651 | Training loss: 0.996590| lrm: 0.549923| num_tokens: 8,932
Step 00294/00651 | Training loss: 1.227482| lrm: 0.548387| num_tokens: 9,845
Step 00295/00651 | Training loss: 0.875439| lrm: 0.546851| num_tokens: 8,584
Step 00296/00651 | Training loss: 0.721567| lrm: 0.545315| num_tokens: 9,170
Step 00297/00651 | Training loss: 0.599856| lrm: 0.543779| num_tokens: 8,077
Step 00298/00651 | Training loss: 0.767020| lrm: 0.542243| num_tokens: 9,519
Step 00299/00651 | Training loss: 0.645244| lrm: 0.540707| num_tokens: 11,549
Step 00300 | Validation loss: 1.069729
Step 00300/00651 | Training loss: 1.071899| lrm: 0.539171| num_tokens: 7,838
Step 00301/00651 | Training loss: 1.329674| lrm: 0.537634| num_tokens: 9,767
Step 00302/00651 | Training loss: 1.035066| lrm: 0.536098| num_tokens: 11,227
Step 00303/00651 | Training loss: 1.353059| lrm: 0.534562| num_tokens: 10,574
Step 00304/00651 | Training loss: 0.985147| lrm: 0.533026| num_tokens: 15,081
Step 00305/00651 | Training loss: 0.985104| lrm: 0.531490| num_tokens: 14,289
Step 00306/00651 | Training loss: 0.927261| lrm: 0.529954| num_tokens: 9,116
Step 00307/00651 | Training loss: 0.900222| lrm: 0.528418| num_tokens: 9,823
Step 00308/00651 | Training loss: 0.897695| lrm: 0.526882| num_tokens: 12,509
Step 00309/00651 | Training loss: 1.039365| lrm: 0.525346| num_tokens: 10,441
Step 00310/00651 | Training loss: 0.975183| lrm: 0.523810| num_tokens: 12,492
Step 00311/00651 | Training loss: 1.289623| lrm: 0.522273| num_tokens: 10,566
Step 00312/00651 | Training loss: 0.685542| lrm: 0.520737| num_tokens: 8,269
Step 00313/00651 | Training loss: 1.196548| lrm: 0.519201| num_tokens: 10,548
Step 00314/00651 | Training loss: 1.347343| lrm: 0.517665| num_tokens: 12,718
Step 00315/00651 | Training loss: 1.512947| lrm: 0.516129| num_tokens: 13,971
Step 00316/00651 | Training loss: 0.451568| lrm: 0.514593| num_tokens: 8,876
Step 00317/00651 | Training loss: 0.652519| lrm: 0.513057| num_tokens: 9,936
Step 00318/00651 | Training loss: 1.028991| lrm: 0.511521| num_tokens: 9,716
Step 00319/00651 | Training loss: 0.930370| lrm: 0.509985| num_tokens: 11,766
Step 00320/00651 | Training loss: 0.792601| lrm: 0.508449| num_tokens: 10,390
Step 00321/00651 | Training loss: 0.471303| lrm: 0.506912| num_tokens: 9,819
Step 00322/00651 | Training loss: 0.789905| lrm: 0.505376| num_tokens: 10,325
Step 00323/00651 | Training loss: 0.918772| lrm: 0.503840| num_tokens: 12,205
Step 00324/00651 | Training loss: 1.220068| lrm: 0.502304| num_tokens: 8,114
Step 00325/00651 | Training loss: 0.849093| lrm: 0.500768| num_tokens: 10,962
Step 00326/00651 | Training loss: 0.820948| lrm: 0.499232| num_tokens: 16,278
Step 00327/00651 | Training loss: 1.132358| lrm: 0.497696| num_tokens: 9,293
Step 00328/00651 | Training loss: 0.933358| lrm: 0.496160| num_tokens: 14,765
Step 00329/00651 | Training loss: 0.689635| lrm: 0.494624| num_tokens: 13,534
Step 00330/00651 | Training loss: 0.798347| lrm: 0.493088| num_tokens: 7,887
Step 00331/00651 | Training loss: 0.806874| lrm: 0.491551| num_tokens: 14,772
Step 00332/00651 | Training loss: 1.253956| lrm: 0.490015| num_tokens: 8,949
Step 00333/00651 | Training loss: 1.000529| lrm: 0.488479| num_tokens: 6,281
Step 00334/00651 | Training loss: 1.270696| lrm: 0.486943| num_tokens: 14,353
Step 00335/00651 | Training loss: 0.541201| lrm: 0.485407| num_tokens: 12,910
Step 00336/00651 | Training loss: 0.964006| lrm: 0.483871| num_tokens: 7,114
Step 00337/00651 | Training loss: 1.188782| lrm: 0.482335| num_tokens: 9,616
Step 00338/00651 | Training loss: 1.180329| lrm: 0.480799| num_tokens: 12,105
Step 00339/00651 | Training loss: 1.282299| lrm: 0.479263| num_tokens: 14,102
Step 00340/00651 | Training loss: 0.552238| lrm: 0.477727| num_tokens: 11,990
Step 00341/00651 | Training loss: 1.141753| lrm: 0.476190| num_tokens: 10,125
Step 00342/00651 | Training loss: 1.175997| lrm: 0.474654| num_tokens: 10,496
Step 00343/00651 | Training loss: 0.769717| lrm: 0.473118| num_tokens: 6,185
Step 00344/00651 | Training loss: 0.862438| lrm: 0.471582| num_tokens: 8,691
Step 00345/00651 | Training loss: 0.436808| lrm: 0.470046| num_tokens: 8,041
Step 00346/00651 | Training loss: 0.822664| lrm: 0.468510| num_tokens: 8,097
Step 00347/00651 | Training loss: 0.646458| lrm: 0.466974| num_tokens: 9,350
Step 00348/00651 | Training loss: 1.211878| lrm: 0.465438| num_tokens: 11,048
Step 00349/00651 | Training loss: 0.470552| lrm: 0.463902| num_tokens: 13,213
Step 00350/00651 | Training loss: 1.050443| lrm: 0.462366| num_tokens: 12,277
Step 00351/00651 | Training loss: 1.311273| lrm: 0.460829| num_tokens: 6,449
Step 00352/00651 | Training loss: 0.958454| lrm: 0.459293| num_tokens: 13,217
Step 00353/00651 | Training loss: 0.869947| lrm: 0.457757| num_tokens: 8,545
Step 00354/00651 | Training loss: 1.165552| lrm: 0.456221| num_tokens: 9,525
Step 00355/00651 | Training loss: 0.692572| lrm: 0.454685| num_tokens: 9,315
Step 00356/00651 | Training loss: 0.674074| lrm: 0.453149| num_tokens: 8,426
Step 00357/00651 | Training loss: 0.813298| lrm: 0.451613| num_tokens: 13,191
Step 00358/00651 | Training loss: 0.874603| lrm: 0.450077| num_tokens: 11,093
Step 00359/00651 | Training loss: 0.556062| lrm: 0.448541| num_tokens: 8,999
Step 00360/00651 | Training loss: 1.679258| lrm: 0.447005| num_tokens: 6,585
Step 00361/00651 | Training loss: 0.913603| lrm: 0.445469| num_tokens: 11,482
Step 00362/00651 | Training loss: 1.052107| lrm: 0.443932| num_tokens: 12,446
Step 00363/00651 | Training loss: 0.974584| lrm: 0.442396| num_tokens: 8,766
Step 00364/00651 | Training loss: 0.878781| lrm: 0.440860| num_tokens: 14,382
Step 00365/00651 | Training loss: 0.993632| lrm: 0.439324| num_tokens: 14,276
Step 00366/00651 | Training loss: 1.141225| lrm: 0.437788| num_tokens: 8,546
Step 00367/00651 | Training loss: 1.188891| lrm: 0.436252| num_tokens: 10,783
Step 00368/00651 | Training loss: 0.671452| lrm: 0.434716| num_tokens: 7,468
Step 00369/00651 | Training loss: 0.960522| lrm: 0.433180| num_tokens: 10,116
Step 00370/00651 | Training loss: 0.702156| lrm: 0.431644| num_tokens: 8,552
Step 00371/00651 | Training loss: 1.126857| lrm: 0.430108| num_tokens: 12,665
Step 00372/00651 | Training loss: 0.650786| lrm: 0.428571| num_tokens: 11,390
Step 00373/00651 | Training loss: 1.200155| lrm: 0.427035| num_tokens: 7,813
Step 00374/00651 | Training loss: 1.003894| lrm: 0.425499| num_tokens: 15,760
Step 00375/00651 | Training loss: 1.002924| lrm: 0.423963| num_tokens: 11,017
Step 00376/00651 | Training loss: 1.377162| lrm: 0.422427| num_tokens: 10,578
Step 00377/00651 | Training loss: 0.746730| lrm: 0.420891| num_tokens: 10,171
Step 00378/00651 | Training loss: 1.061973| lrm: 0.419355| num_tokens: 9,970
Step 00379/00651 | Training loss: 0.870656| lrm: 0.417819| num_tokens: 7,815
Step 00380/00651 | Training loss: 0.691192| lrm: 0.416283| num_tokens: 13,469
Step 00381/00651 | Training loss: 1.147976| lrm: 0.414747| num_tokens: 11,789
Step 00382/00651 | Training loss: 0.935652| lrm: 0.413210| num_tokens: 9,833
Step 00383/00651 | Training loss: 1.089317| lrm: 0.411674| num_tokens: 14,081
Step 00384/00651 | Training loss: 0.995131| lrm: 0.410138| num_tokens: 12,580
Step 00385/00651 | Training loss: 1.371224| lrm: 0.408602| num_tokens: 10,170
Step 00386/00651 | Training loss: 0.648542| lrm: 0.407066| num_tokens: 7,599
Step 00387/00651 | Training loss: 0.959069| lrm: 0.405530| num_tokens: 10,037
Step 00388/00651 | Training loss: 0.970433| lrm: 0.403994| num_tokens: 11,968
Step 00389/00651 | Training loss: 1.122852| lrm: 0.402458| num_tokens: 8,372
Step 00390/00651 | Training loss: 0.822857| lrm: 0.400922| num_tokens: 11,714
Step 00391/00651 | Training loss: 1.148022| lrm: 0.399386| num_tokens: 8,928
Step 00392/00651 | Training loss: 0.971854| lrm: 0.397849| num_tokens: 11,389
Step 00393/00651 | Training loss: 1.090023| lrm: 0.396313| num_tokens: 9,085
Step 00394/00651 | Training loss: 0.777026| lrm: 0.394777| num_tokens: 9,093
Step 00395/00651 | Training loss: 1.397192| lrm: 0.393241| num_tokens: 9,032
Step 00396/00651 | Training loss: 0.890158| lrm: 0.391705| num_tokens: 7,210
Step 00397/00651 | Training loss: 0.696495| lrm: 0.390169| num_tokens: 8,582
Step 00398/00651 | Training loss: 1.220598| lrm: 0.388633| num_tokens: 10,891
Step 00399/00651 | Training loss: 0.642881| lrm: 0.387097| num_tokens: 9,308
Step 00400 | Validation loss: 1.068968
Final: 245/1024 (23.93%)
Final: 261/1024 (25.49%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 7/64 (10.94%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 1/64 (1.56%)
Step 00400 | mmlu_acc: 0.239258, arc_easy_acc: 0.254883, gsm8k_acc: 0.109375, humaneval_acc: 0.015625
Step 00400/00651 | Training loss: 0.907645| lrm: 0.385561| num_tokens: 10,073
Step 00401/00651 | Training loss: 0.560275| lrm: 0.384025| num_tokens: 7,290
Step 00402/00651 | Training loss: 0.772573| lrm: 0.382488| num_tokens: 11,264
Step 00403/00651 | Training loss: 0.808047| lrm: 0.380952| num_tokens: 11,204
Step 00404/00651 | Training loss: 0.815380| lrm: 0.379416| num_tokens: 11,062
Step 00405/00651 | Training loss: 0.895927| lrm: 0.377880| num_tokens: 9,963
Step 00406/00651 | Training loss: 1.112255| lrm: 0.376344| num_tokens: 16,285
Step 00407/00651 | Training loss: 1.182940| lrm: 0.374808| num_tokens: 13,579
Step 00408/00651 | Training loss: 0.910006| lrm: 0.373272| num_tokens: 7,913
Step 00409/00651 | Training loss: 0.676774| lrm: 0.371736| num_tokens: 8,980
Step 00410/00651 | Training loss: 0.670857| lrm: 0.370200| num_tokens: 12,469
Step 00411/00651 | Training loss: 1.010185| lrm: 0.368664| num_tokens: 14,290
Step 00412/00651 | Training loss: 1.294098| lrm: 0.367127| num_tokens: 11,578
Step 00413/00651 | Training loss: 0.990398| lrm: 0.365591| num_tokens: 10,583
Step 00414/00651 | Training loss: 0.879095| lrm: 0.364055| num_tokens: 10,272
Step 00415/00651 | Training loss: 1.545949| lrm: 0.362519| num_tokens: 12,059
Step 00416/00651 | Training loss: 0.762071| lrm: 0.360983| num_tokens: 7,262
Step 00417/00651 | Training loss: 0.902178| lrm: 0.359447| num_tokens: 9,760
Step 00418/00651 | Training loss: 0.888994| lrm: 0.357911| num_tokens: 8,571
Step 00419/00651 | Training loss: 0.575168| lrm: 0.356375| num_tokens: 10,530
Step 00420/00651 | Training loss: 1.576480| lrm: 0.354839| num_tokens: 9,809
Step 00421/00651 | Training loss: 0.949846| lrm: 0.353303| num_tokens: 12,362
Step 00422/00651 | Training loss: 1.205866| lrm: 0.351767| num_tokens: 10,487
Step 00423/00651 | Training loss: 1.207432| lrm: 0.350230| num_tokens: 10,498
Step 00424/00651 | Training loss: 1.124836| lrm: 0.348694| num_tokens: 17,589
Step 00425/00651 | Training loss: 0.801505| lrm: 0.347158| num_tokens: 7,363
Step 00426/00651 | Training loss: 1.132689| lrm: 0.345622| num_tokens: 11,244
Step 00427/00651 | Training loss: 0.793860| lrm: 0.344086| num_tokens: 11,909
Step 00428/00651 | Training loss: 0.960438| lrm: 0.342550| num_tokens: 10,213
Step 00429/00651 | Training loss: 1.019515| lrm: 0.341014| num_tokens: 10,694
Step 00430/00651 | Training loss: 1.254996| lrm: 0.339478| num_tokens: 13,365
Step 00431/00651 | Training loss: 0.783448| lrm: 0.337942| num_tokens: 15,624
Step 00432/00651 | Training loss: 1.090002| lrm: 0.336406| num_tokens: 12,413
Step 00433/00651 | Training loss: 0.901706| lrm: 0.334869| num_tokens: 5,852
Step 00434/00651 | Training loss: 1.010349| lrm: 0.333333| num_tokens: 12,767
Step 00435/00651 | Training loss: 1.167329| lrm: 0.331797| num_tokens: 10,626
Step 00436/00651 | Training loss: 1.194542| lrm: 0.330261| num_tokens: 11,416
Step 00437/00651 | Training loss: 0.507497| lrm: 0.328725| num_tokens: 6,887
Step 00438/00651 | Training loss: 1.013287| lrm: 0.327189| num_tokens: 11,745
Step 00439/00651 | Training loss: 0.851418| lrm: 0.325653| num_tokens: 8,343
Step 00440/00651 | Training loss: 0.405688| lrm: 0.324117| num_tokens: 9,943
Step 00441/00651 | Training loss: 0.602684| lrm: 0.322581| num_tokens: 7,433
Step 00442/00651 | Training loss: 0.643502| lrm: 0.321045| num_tokens: 10,064
Step 00443/00651 | Training loss: 1.651376| lrm: 0.319508| num_tokens: 15,540
Step 00444/00651 | Training loss: 0.683324| lrm: 0.317972| num_tokens: 7,449
Step 00445/00651 | Training loss: 1.094073| lrm: 0.316436| num_tokens: 8,793
Step 00446/00651 | Training loss: 1.065759| lrm: 0.314900| num_tokens: 8,479
Step 00447/00651 | Training loss: 1.252602| lrm: 0.313364| num_tokens: 12,295
Step 00448/00651 | Training loss: 0.987018| lrm: 0.311828| num_tokens: 13,139
Step 00449/00651 | Training loss: 1.159249| lrm: 0.310292| num_tokens: 12,922
Step 00450/00651 | Training loss: 0.649409| lrm: 0.308756| num_tokens: 12,352
Step 00451/00651 | Training loss: 1.323181| lrm: 0.307220| num_tokens: 11,883
Step 00452/00651 | Training loss: 0.882468| lrm: 0.305684| num_tokens: 13,822
Step 00453/00651 | Training loss: 0.565324| lrm: 0.304147| num_tokens: 10,512
Step 00454/00651 | Training loss: 0.964105| lrm: 0.302611| num_tokens: 10,986
Step 00455/00651 | Training loss: 0.790192| lrm: 0.301075| num_tokens: 5,676
Step 00456/00651 | Training loss: 1.076751| lrm: 0.299539| num_tokens: 10,055
Step 00457/00651 | Training loss: 1.396657| lrm: 0.298003| num_tokens: 11,421
Step 00458/00651 | Training loss: 1.172022| lrm: 0.296467| num_tokens: 14,130
Step 00459/00651 | Training loss: 1.019087| lrm: 0.294931| num_tokens: 11,707
Step 00460/00651 | Training loss: 0.866893| lrm: 0.293395| num_tokens: 8,864
Step 00461/00651 | Training loss: 1.092592| lrm: 0.291859| num_tokens: 8,653
Step 00462/00651 | Training loss: 1.644684| lrm: 0.290323| num_tokens: 10,225
Step 00463/00651 | Training loss: 1.252943| lrm: 0.288786| num_tokens: 11,249
Step 00464/00651 | Training loss: 0.972660| lrm: 0.287250| num_tokens: 13,028
Step 00465/00651 | Training loss: 0.614085| lrm: 0.285714| num_tokens: 13,322
Step 00466/00651 | Training loss: 1.337788| lrm: 0.284178| num_tokens: 7,475
Step 00467/00651 | Training loss: 0.815303| lrm: 0.282642| num_tokens: 13,710
Step 00468/00651 | Training loss: 1.028521| lrm: 0.281106| num_tokens: 11,363
Step 00469/00651 | Training loss: 1.439249| lrm: 0.279570| num_tokens: 13,923
Step 00470/00651 | Training loss: 0.961447| lrm: 0.278034| num_tokens: 11,582
Step 00471/00651 | Training loss: 0.665914| lrm: 0.276498| num_tokens: 9,379
Step 00472/00651 | Training loss: 0.690734| lrm: 0.274962| num_tokens: 13,789
Step 00473/00651 | Training loss: 0.729350| lrm: 0.273425| num_tokens: 13,789
Step 00474/00651 | Training loss: 1.401404| lrm: 0.271889| num_tokens: 11,818
Step 00475/00651 | Training loss: 0.942579| lrm: 0.270353| num_tokens: 13,827
Step 00476/00651 | Training loss: 0.400574| lrm: 0.268817| num_tokens: 10,961
Step 00477/00651 | Training loss: 0.538655| lrm: 0.267281| num_tokens: 9,912
Step 00478/00651 | Training loss: 0.746229| lrm: 0.265745| num_tokens: 11,052
Step 00479/00651 | Training loss: 0.320349| lrm: 0.264209| num_tokens: 9,722
Step 00480/00651 | Training loss: 1.225608| lrm: 0.262673| num_tokens: 10,451
Step 00481/00651 | Training loss: 0.634012| lrm: 0.261137| num_tokens: 11,121
Step 00482/00651 | Training loss: 0.778418| lrm: 0.259601| num_tokens: 14,708
Step 00483/00651 | Training loss: 0.964643| lrm: 0.258065| num_tokens: 10,918
Step 00484/00651 | Training loss: 1.301634| lrm: 0.256528| num_tokens: 11,029
Step 00485/00651 | Training loss: 1.037823| lrm: 0.254992| num_tokens: 16,816
Step 00486/00651 | Training loss: 0.912780| lrm: 0.253456| num_tokens: 10,517
Step 00487/00651 | Training loss: 0.946862| lrm: 0.251920| num_tokens: 8,583
Step 00488/00651 | Training loss: 1.081493| lrm: 0.250384| num_tokens: 7,142
Step 00489/00651 | Training loss: 0.988555| lrm: 0.248848| num_tokens: 10,809
Step 00490/00651 | Training loss: 1.127991| lrm: 0.247312| num_tokens: 11,375
Step 00491/00651 | Training loss: 0.838521| lrm: 0.245776| num_tokens: 12,637
Step 00492/00651 | Training loss: 0.960942| lrm: 0.244240| num_tokens: 8,545
Step 00493/00651 | Training loss: 1.294168| lrm: 0.242704| num_tokens: 8,826
Step 00494/00651 | Training loss: 0.502707| lrm: 0.241167| num_tokens: 7,908
Step 00495/00651 | Training loss: 0.918668| lrm: 0.239631| num_tokens: 10,708
Step 00496/00651 | Training loss: 0.956545| lrm: 0.238095| num_tokens: 14,726
Step 00497/00651 | Training loss: 1.411555| lrm: 0.236559| num_tokens: 11,360
Step 00498/00651 | Training loss: 0.692254| lrm: 0.235023| num_tokens: 7,256
Step 00499/00651 | Training loss: 1.146978| lrm: 0.233487| num_tokens: 14,012
Step 00500 | Validation loss: 1.067829
Step 00500/00651 | Training loss: 0.573172| lrm: 0.231951| num_tokens: 10,712
Step 00501/00651 | Training loss: 1.173057| lrm: 0.230415| num_tokens: 14,308
Step 00502/00651 | Training loss: 1.040795| lrm: 0.228879| num_tokens: 7,880
Step 00503/00651 | Training loss: 0.972405| lrm: 0.227343| num_tokens: 9,391
Step 00504/00651 | Training loss: 0.520324| lrm: 0.225806| num_tokens: 10,994
Step 00505/00651 | Training loss: 0.628122| lrm: 0.224270| num_tokens: 11,523
Step 00506/00651 | Training loss: 1.226599| lrm: 0.222734| num_tokens: 7,724
Step 00507/00651 | Training loss: 1.126779| lrm: 0.221198| num_tokens: 11,640
Step 00508/00651 | Training loss: 1.028573| lrm: 0.219662| num_tokens: 9,591
Step 00509/00651 | Training loss: 0.775600| lrm: 0.218126| num_tokens: 10,731
Step 00510/00651 | Training loss: 1.023752| lrm: 0.216590| num_tokens: 14,449
Step 00511/00651 | Training loss: 1.219722| lrm: 0.215054| num_tokens: 10,389
Step 00512/00651 | Training loss: 1.221398| lrm: 0.213518| num_tokens: 11,529
Step 00513/00651 | Training loss: 0.840151| lrm: 0.211982| num_tokens: 8,959
Step 00514/00651 | Training loss: 0.445172| lrm: 0.210445| num_tokens: 9,971
Step 00515/00651 | Training loss: 0.710084| lrm: 0.208909| num_tokens: 12,321
Step 00516/00651 | Training loss: 1.256254| lrm: 0.207373| num_tokens: 10,747
Step 00517/00651 | Training loss: 0.885479| lrm: 0.205837| num_tokens: 9,898
Step 00518/00651 | Training loss: 0.861557| lrm: 0.204301| num_tokens: 15,247
Step 00519/00651 | Training loss: 1.133583| lrm: 0.202765| num_tokens: 6,956
Step 00520/00651 | Training loss: 0.687503| lrm: 0.201229| num_tokens: 10,882
Step 00521/00651 | Training loss: 0.906388| lrm: 0.199693| num_tokens: 11,521
Step 00522/00651 | Training loss: 0.890479| lrm: 0.198157| num_tokens: 9,685
Step 00523/00651 | Training loss: 0.835134| lrm: 0.196621| num_tokens: 12,660
Step 00524/00651 | Training loss: 0.738057| lrm: 0.195084| num_tokens: 11,353
Step 00525/00651 | Training loss: 0.710487| lrm: 0.193548| num_tokens: 15,204
Step 00526/00651 | Training loss: 1.098685| lrm: 0.192012| num_tokens: 16,031
Step 00527/00651 | Training loss: 0.983895| lrm: 0.190476| num_tokens: 19,728
Step 00528/00651 | Training loss: 0.783321| lrm: 0.188940| num_tokens: 7,287
Step 00529/00651 | Training loss: 0.748182| lrm: 0.187404| num_tokens: 10,687
Step 00530/00651 | Training loss: 0.962070| lrm: 0.185868| num_tokens: 12,765
Step 00531/00651 | Training loss: 0.632694| lrm: 0.184332| num_tokens: 10,131
Step 00532/00651 | Training loss: 0.823185| lrm: 0.182796| num_tokens: 8,751
Step 00533/00651 | Training loss: 0.752132| lrm: 0.181260| num_tokens: 13,672
Step 00534/00651 | Training loss: 0.518868| lrm: 0.179724| num_tokens: 12,219
Step 00535/00651 | Training loss: 0.985933| lrm: 0.178187| num_tokens: 9,719
Step 00536/00651 | Training loss: 0.910232| lrm: 0.176651| num_tokens: 12,255
Step 00537/00651 | Training loss: 0.633140| lrm: 0.175115| num_tokens: 13,072
Step 00538/00651 | Training loss: 1.053640| lrm: 0.173579| num_tokens: 10,457
Step 00539/00651 | Training loss: 1.316596| lrm: 0.172043| num_tokens: 8,715
Step 00540/00651 | Training loss: 0.525228| lrm: 0.170507| num_tokens: 14,386
Step 00541/00651 | Training loss: 0.935881| lrm: 0.168971| num_tokens: 8,558
Step 00542/00651 | Training loss: 1.043892| lrm: 0.167435| num_tokens: 14,572
Step 00543/00651 | Training loss: 0.935954| lrm: 0.165899| num_tokens: 8,901
Step 00544/00651 | Training loss: 0.943311| lrm: 0.164363| num_tokens: 16,175
Step 00545/00651 | Training loss: 0.885116| lrm: 0.162826| num_tokens: 7,696
Step 00546/00651 | Training loss: 1.059167| lrm: 0.161290| num_tokens: 11,135
Step 00547/00651 | Training loss: 0.980252| lrm: 0.159754| num_tokens: 8,534
Step 00548/00651 | Training loss: 1.320947| lrm: 0.158218| num_tokens: 10,877
Step 00549/00651 | Training loss: 0.989183| lrm: 0.156682| num_tokens: 10,465
Step 00550/00651 | Training loss: 1.214776| lrm: 0.155146| num_tokens: 12,791
Step 00551/00651 | Training loss: 0.777513| lrm: 0.153610| num_tokens: 9,183
Step 00552/00651 | Training loss: 0.983489| lrm: 0.152074| num_tokens: 11,900
Step 00553/00651 | Training loss: 1.014207| lrm: 0.150538| num_tokens: 12,202
Step 00554/00651 | Training loss: 0.439432| lrm: 0.149002| num_tokens: 9,090
Step 00555/00651 | Training loss: 1.069553| lrm: 0.147465| num_tokens: 10,500
Step 00556/00651 | Training loss: 0.893277| lrm: 0.145929| num_tokens: 11,324
Step 00557/00651 | Training loss: 1.279097| lrm: 0.144393| num_tokens: 8,102
Step 00558/00651 | Training loss: 0.853885| lrm: 0.142857| num_tokens: 10,915
Step 00559/00651 | Training loss: 0.978012| lrm: 0.141321| num_tokens: 9,324
Step 00560/00651 | Training loss: 0.223115| lrm: 0.139785| num_tokens: 9,535
Step 00561/00651 | Training loss: 1.247950| lrm: 0.138249| num_tokens: 10,248
Step 00562/00651 | Training loss: 0.743096| lrm: 0.136713| num_tokens: 10,463
Step 00563/00651 | Training loss: 0.991471| lrm: 0.135177| num_tokens: 7,877
Step 00564/00651 | Training loss: 0.514584| lrm: 0.133641| num_tokens: 12,522
Step 00565/00651 | Training loss: 1.003192| lrm: 0.132104| num_tokens: 14,793
Step 00566/00651 | Training loss: 0.992476| lrm: 0.130568| num_tokens: 10,581
Step 00567/00651 | Training loss: 0.724733| lrm: 0.129032| num_tokens: 12,754
Step 00568/00651 | Training loss: 0.929678| lrm: 0.127496| num_tokens: 6,278
Step 00569/00651 | Training loss: 0.932283| lrm: 0.125960| num_tokens: 13,728
Step 00570/00651 | Training loss: 1.253274| lrm: 0.124424| num_tokens: 13,497
Step 00571/00651 | Training loss: 1.152145| lrm: 0.122888| num_tokens: 15,936
Step 00572/00651 | Training loss: 1.166051| lrm: 0.121352| num_tokens: 9,123
Step 00573/00651 | Training loss: 1.375310| lrm: 0.119816| num_tokens: 9,414
Step 00574/00651 | Training loss: 0.824101| lrm: 0.118280| num_tokens: 10,295
Step 00575/00651 | Training loss: 0.738937| lrm: 0.116743| num_tokens: 8,542
Step 00576/00651 | Training loss: 0.661681| lrm: 0.115207| num_tokens: 10,852
Step 00577/00651 | Training loss: 0.745143| lrm: 0.113671| num_tokens: 8,394
Step 00578/00651 | Training loss: 1.514002| lrm: 0.112135| num_tokens: 8,758
Step 00579/00651 | Training loss: 1.090665| lrm: 0.110599| num_tokens: 16,450
Step 00580/00651 | Training loss: 1.128413| lrm: 0.109063| num_tokens: 12,478
Step 00581/00651 | Training loss: 0.824968| lrm: 0.107527| num_tokens: 10,075
Step 00582/00651 | Training loss: 0.849169| lrm: 0.105991| num_tokens: 11,969
Step 00583/00651 | Training loss: 0.565549| lrm: 0.104455| num_tokens: 7,089
Step 00584/00651 | Training loss: 1.303975| lrm: 0.102919| num_tokens: 10,669
Step 00585/00651 | Training loss: 1.162457| lrm: 0.101382| num_tokens: 13,253
Step 00586/00651 | Training loss: 0.805936| lrm: 0.099846| num_tokens: 10,966
Step 00587/00651 | Training loss: 0.614315| lrm: 0.098310| num_tokens: 13,931
Step 00588/00651 | Training loss: 1.054767| lrm: 0.096774| num_tokens: 9,468
Step 00589/00651 | Training loss: 0.673892| lrm: 0.095238| num_tokens: 9,535
Step 00590/00651 | Training loss: 0.835398| lrm: 0.093702| num_tokens: 12,773
Step 00591/00651 | Training loss: 1.053022| lrm: 0.092166| num_tokens: 9,241
Step 00592/00651 | Training loss: 1.692675| lrm: 0.090630| num_tokens: 16,289
Step 00593/00651 | Training loss: 0.508494| lrm: 0.089094| num_tokens: 7,273
Step 00594/00651 | Training loss: 1.140468| lrm: 0.087558| num_tokens: 14,089
Step 00595/00651 | Training loss: 1.247285| lrm: 0.086022| num_tokens: 15,925
Step 00596/00651 | Training loss: 0.968701| lrm: 0.084485| num_tokens: 8,647
Step 00597/00651 | Training loss: 0.624304| lrm: 0.082949| num_tokens: 12,816
Step 00598/00651 | Training loss: 1.040488| lrm: 0.081413| num_tokens: 8,478
Step 00599/00651 | Training loss: 2.192353| lrm: 0.079877| num_tokens: 9,830
Step 00600 | Validation loss: 1.067409
Final: 261/1024 (25.49%)
Final: 280/1024 (27.34%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 6/64 (9.38%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 1/64 (1.56%)
Step 00600 | mmlu_acc: 0.254883, arc_easy_acc: 0.273438, gsm8k_acc: 0.093750, humaneval_acc: 0.015625
Step 00600/00651 | Training loss: 0.740190| lrm: 0.078341| num_tokens: 15,797
Step 00601/00651 | Training loss: 1.419528| lrm: 0.076805| num_tokens: 7,591
Step 00602/00651 | Training loss: 0.527175| lrm: 0.075269| num_tokens: 11,169
Step 00603/00651 | Training loss: 0.625143| lrm: 0.073733| num_tokens: 10,358
Step 00604/00651 | Training loss: 1.125315| lrm: 0.072197| num_tokens: 8,964
Step 00605/00651 | Training loss: 1.006662| lrm: 0.070661| num_tokens: 14,173
Step 00606/00651 | Training loss: 1.138191| lrm: 0.069124| num_tokens: 9,796
Step 00607/00651 | Training loss: 0.734678| lrm: 0.067588| num_tokens: 10,658
Step 00608/00651 | Training loss: 0.904281| lrm: 0.066052| num_tokens: 9,914
Step 00609/00651 | Training loss: 0.866777| lrm: 0.064516| num_tokens: 11,189
Step 00610/00651 | Training loss: 0.836755| lrm: 0.062980| num_tokens: 10,263
Step 00611/00651 | Training loss: 0.904722| lrm: 0.061444| num_tokens: 10,448
Step 00612/00651 | Training loss: 0.577371| lrm: 0.059908| num_tokens: 10,952
Step 00613/00651 | Training loss: 1.387831| lrm: 0.058372| num_tokens: 10,614
Step 00614/00651 | Training loss: 0.941245| lrm: 0.056836| num_tokens: 8,594
Step 00615/00651 | Training loss: 0.799709| lrm: 0.055300| num_tokens: 9,510
Step 00616/00651 | Training loss: 0.933990| lrm: 0.053763| num_tokens: 8,463
Step 00617/00651 | Training loss: 0.850922| lrm: 0.052227| num_tokens: 10,051
Step 00618/00651 | Training loss: 0.988699| lrm: 0.050691| num_tokens: 9,561
Step 00619/00651 | Training loss: 0.864451| lrm: 0.049155| num_tokens: 12,420
Step 00620/00651 | Training loss: 0.700609| lrm: 0.047619| num_tokens: 12,121
Step 00621/00651 | Training loss: 0.750393| lrm: 0.046083| num_tokens: 10,035
Step 00622/00651 | Training loss: 0.888528| lrm: 0.044547| num_tokens: 11,160
Step 00623/00651 | Training loss: 1.361608| lrm: 0.043011| num_tokens: 12,840
Step 00624/00651 | Training loss: 0.603386| lrm: 0.041475| num_tokens: 8,208
Step 00625/00651 | Training loss: 1.073857| lrm: 0.039939| num_tokens: 9,824
Step 00626/00651 | Training loss: 1.090186| lrm: 0.038402| num_tokens: 7,908
Step 00627/00651 | Training loss: 0.614522| lrm: 0.036866| num_tokens: 11,913
Step 00628/00651 | Training loss: 0.784431| lrm: 0.035330| num_tokens: 12,453
Step 00629/00651 | Training loss: 1.167070| lrm: 0.033794| num_tokens: 13,882
Step 00630/00651 | Training loss: 1.184375| lrm: 0.032258| num_tokens: 11,986
Step 00631/00651 | Training loss: 0.460805| lrm: 0.030722| num_tokens: 7,541
Step 00632/00651 | Training loss: 1.229032| lrm: 0.029186| num_tokens: 16,830
Step 00633/00651 | Training loss: 0.641908| lrm: 0.027650| num_tokens: 8,288
Step 00634/00651 | Training loss: 0.806788| lrm: 0.026114| num_tokens: 11,096
Step 00635/00651 | Training loss: 0.819370| lrm: 0.024578| num_tokens: 10,802
Step 00636/00651 | Training loss: 0.694189| lrm: 0.023041| num_tokens: 9,890
Step 00637/00651 | Training loss: 0.601532| lrm: 0.021505| num_tokens: 10,192
Step 00638/00651 | Training loss: 0.902885| lrm: 0.019969| num_tokens: 12,554
Step 00639/00651 | Training loss: 1.200251| lrm: 0.018433| num_tokens: 10,390
Step 00640/00651 | Training loss: 0.917432| lrm: 0.016897| num_tokens: 12,822
Step 00641/00651 | Training loss: 0.569727| lrm: 0.015361| num_tokens: 8,306
Step 00642/00651 | Training loss: 1.066669| lrm: 0.013825| num_tokens: 10,058
Step 00643/00651 | Training loss: 1.048385| lrm: 0.012289| num_tokens: 12,415
Step 00644/00651 | Training loss: 1.591997| lrm: 0.010753| num_tokens: 8,549
Step 00645/00651 | Training loss: 1.260044| lrm: 0.009217| num_tokens: 12,361
Step 00646/00651 | Training loss: 0.811328| lrm: 0.007680| num_tokens: 9,268
Step 00647/00651 | Training loss: 0.749003| lrm: 0.006144| num_tokens: 12,746
Step 00648/00651 | Training loss: 1.444624| lrm: 0.004608| num_tokens: 7,389
Step 00649/00651 | Training loss: 1.211644| lrm: 0.003072| num_tokens: 11,668
Step 00650 | Validation loss: 1.067340
Final: 252/1024 (24.61%)
Final: 271/1024 (26.46%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 6/64 (9.38%)
[KRank 0 | 0/8 (0.00%)
==================================================
Final: 1/64 (1.56%)
Step 00650 | mmlu_acc: 0.246094, arc_easy_acc: 0.264648, gsm8k_acc: 0.093750, humaneval_acc: 0.015625
2025-10-15 18:56:13,330 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model file to: /root/.cache/nanochat/chatsft_checkpoints/d20/model_000650.pt
2025-10-15 18:56:13,331 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata file to: /root/.cache/nanochat/chatsft_checkpoints/d20/meta_000650.json
‚úÖ Saved model checkpoint to /root/.cache/nanochat/chatsft_checkpoints/d20
